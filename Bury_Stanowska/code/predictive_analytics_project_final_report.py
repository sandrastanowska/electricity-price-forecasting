# -*- coding: utf-8 -*-
"""Predictive Analytics project - final report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dYuv8Or1fsREsvnpiPB5b8l3WBMjMXoD

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#On-the-importance-of-the-long-term-seasonal-component-in-day-ahead-electricity-price-forecasting-with-NARX-neural-networks" data-toc-modified-id="On-the-importance-of-the-long-term-seasonal-component-in-day-ahead-electricity-price-forecasting-with-NARX-neural-networks-1"><span class="toc-item-num">1&nbsp;&nbsp;</span><strong>On the importance of the long-term seasonal component in day-ahead electricity price forecasting with NARX neural networks</strong></a></span><ul class="toc-item"><li><span><a href="#Replication-of-the-original-paper-results" data-toc-modified-id="Replication-of-the-original-paper-results-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Replication of the original paper results</a></span><ul class="toc-item"><li><span><a href="#The-naïve-and-expert-benchmarks" data-toc-modified-id="The-naïve-and-expert-benchmarks-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>The naïve and expert benchmarks</a></span></li><li><span><a href="#The-seasonal-component-autoregressive-(SCAR)-benchmarks" data-toc-modified-id="The-seasonal-component-autoregressive-(SCAR)-benchmarks-1.1.2"><span class="toc-item-num">1.1.2&nbsp;&nbsp;</span>The seasonal component autoregressive (SCAR) benchmarks</a></span></li><li><span><a href="#The-artificial-neural-network-(ANN)-benchmark" data-toc-modified-id="The-artificial-neural-network-(ANN)-benchmark-1.1.3"><span class="toc-item-num">1.1.3&nbsp;&nbsp;</span>The artificial neural network (ANN) benchmark</a></span></li><li><span><a href="#Committee-machines-of-ANN-networks" data-toc-modified-id="Committee-machines-of-ANN-networks-1.1.4"><span class="toc-item-num">1.1.4&nbsp;&nbsp;</span>Committee machines of ANN networks</a></span></li><li><span><a href="#Committee-machines-of-SCANN-networks" data-toc-modified-id="Committee-machines-of-SCANN-networks-1.1.5"><span class="toc-item-num">1.1.5&nbsp;&nbsp;</span>Committee machines of SCANN networks</a></span></li></ul></li><li><span><a href="#Empirical-results" data-toc-modified-id="Empirical-results-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>Empirical results</a></span><ul class="toc-item"><li><span><a href="#Performance-evaluation-in-terms-of-WMAE" data-toc-modified-id="Performance-evaluation-in-terms-of-WMAE-1.2.1"><span class="toc-item-num">1.2.1&nbsp;&nbsp;</span>Performance evaluation in terms of WMAE</a></span></li><li><span><a href="#DM-tests" data-toc-modified-id="DM-tests-1.2.2"><span class="toc-item-num">1.2.2&nbsp;&nbsp;</span>DM tests</a></span></li><li><span><a href="#The-number-of-hidden-neurons" data-toc-modified-id="The-number-of-hidden-neurons-1.2.3"><span class="toc-item-num">1.2.3&nbsp;&nbsp;</span>The number of hidden neurons</a></span></li><li><span><a href="#The-size-of-the-committee-machine" data-toc-modified-id="The-size-of-the-committee-machine-1.2.4"><span class="toc-item-num">1.2.4&nbsp;&nbsp;</span>The size of the committee machine</a></span></li></ul></li></ul></li><li><span><a href="#Additional-developement-of-the-paper" data-toc-modified-id="Additional-developement-of-the-paper-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Additional developement of the paper</a></span></li></ul></div>

# **On the importance of the long-term seasonal component in day-ahead electricity price forecasting with NARX neural networks**

**Authors:**

**Bury Nina, 255758**
    
**Stanowska Sandra, 256566**

<h1>Table of Contents<span class="tocSkip"></span></h1>

>[Objectives](#scrollTo=9158eee0)

>[Replication of the original paper results](#scrollTo=24eda3ab)

>>[The naïve and expert benchmarks](#scrollTo=pPhrMPrI-zhg)

>>[The seasonal component autoregressive (SCAR) benchmarks](#scrollTo=65JRJh7T-9NT)

>>[The artificial neural network (ANN) benchmark](#scrollTo=geAzk-vO-41X)

>>[Committee machines of ANN networks](#scrollTo=VFQxywdrAOn7)

>>[Committee machines of SCANN networks](#scrollTo=k4pTnEZ7AT7X)

>[Further developement of the paper](#scrollTo=UIoZiZYOVQXo)

## Replication of the original paper results
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
import datetime as dt
from statsmodels.tsa.filters.hp_filter import hpfilter

plt.rcParams['figure.figsize'] = [15, 10]

from google.colab import files
uploaded = files.upload() # SELECT FILES FROM YOUR COMPUTER

gefcom_data = pd.read_table('GEFCOM.txt', sep="\t", header=None, usecols=[0,1,2,3,4,5],
                            names=['date', 'hour', 'LMP', 'day-ahead zonal load', 'day-ahead system load', 'weekday'])
# gefcom_data = pd.read_table(io.BytesIO(uploaded['GEFCOM.txt']), sep="\t", header=None, usecols=[0,1,2,3,4,5],
#     names=['date', 'hour', 'LMP', 'day-ahead zonal load', 'day-ahead system load', 'weekday'])

np_data = pd.read_table('NPdata_2013-2016.txt', sep='   ', header=None, usecols=[0,1,2,3],
                        names=['date', 'hour', 'system price', 'consumption prognosis', 'weekday'])
# np_data = pd.read_table(io.BytesIO(uploaded['NPdata_2013-2016.txt']), sep='   ', header=None,
# usecols=[0,1,2,3], names=['date', 'hour', 'system price', 'consumption prognosis'])

gefcom_data['date'] = pd.to_datetime(gefcom_data['date'], format='%Y%m%d')
np_data['date'] = pd.to_datetime(np_data['date'], format='%Y%m%d')

gefcom_data

"""In gefcom dataset the weekday is 0 on Sunday."""

np_data

fig, axs = plt.subplots(3, 1,  figsize=(15, 10))
fig.suptitle('GEFCom2014 data', fontsize=16)
axs[0].plot(gefcom_data.LMP)
axs[0].set_ylabel('LMP')
axs[1].plot(gefcom_data['day-ahead zonal load'])
axs[1].set_ylabel('day-ahead zonal load')
axs[2].plot(gefcom_data['day-ahead system load'])
axs[2].set_ylabel('day-ahead system load')

fig, axs = plt.subplots(2, 1,  figsize=(15, 10))
fig.suptitle('Nord Pool data', fontsize=16)
axs[0].plot(np_data['system price'])
axs[0].set_ylabel('system price')
axs[1].plot(np_data['consumption prognosis'])
axs[1].set_ylabel('consumption prognosis')

gefcom_train, gefcom_test = gefcom_data[:360*24], gefcom_data[360*24:]
np_train, np_test = np_data[:360*24], np_data[360*24:]

"""Key terms:

EPS - electricity price forecasting

LTSC - long-term seasonal component

SCAR - Seasonal Component AutoRegressive

NARX - non-linear autoregressive

Check if the data is complete

GEFCOM data
"""

sum(gefcom_data.groupby(['date']).count()['hour'] != 24) # result is 0 so we have complete hours

(gefcom_data['date'] - gefcom_data['date'].shift(24)).describe() #min and max interval is equal to 1 day
    # meaning the data is complete and doesnt lack any days

"""NP DATA"""

sum(np_data.groupby(['date']).count()['hour'] != 24) # result is 0 so we have complete hours

(np_data['date'] - np_data['date'].shift(24)).describe() #min and max interval is equal to 1 day meaning the data
# is complete and doesnt lack any days

gefcom_data['datetime'] = pd.to_datetime(gefcom_data['date'].astype(str) + ' ' + gefcom_data['hour'].astype(int).astype(str).str.zfill(2), format='%Y-%m-%d %H')
np_data['datetime'] = pd.to_datetime(np_data['date'].astype(str) + ' ' + np_data['hour'].astype(int).astype(str).str.zfill(2), format='%Y-%m-%d %H')

np_data['datetime'] = np.where(np_data['hour']==0, np_data['datetime'].shift(-24), np_data['datetime'])

"""### The naïve and expert benchmarks

* NAIVE
"""

# NAIVE
# the price forecast for hour h on Monday is equal to that for hour h on Monday of the previous week, i.e.,
# Pˆd,h = Pd−7,h, and the same rule applies for Saturdays and Sundays.
# The price forecast for hour h on Tuesday is equal to the price for hour h on Monday, i.e.,
# Pˆ d,h = Pd−1,h, and the same rule applies for Wednesdays, Thursdays and Fridays

def naive_forecast(dataset, column_name, forecast_window):
  dataset_train, dataset_test = dataset[:forecast_window*24], dataset[forecast_window*24:]
  # del dataset['naive']
  if 'naive' in dataset.columns:
    del dataset['naive']
  dataset.insert(len(dataset.columns), 'naive', dataset_train[column_name])
  for row in range(forecast_window*24, forecast_window*24 + len(dataset_test)):
    if ((dataset.loc[row, 'weekday'] == 0)|(dataset.loc[row, 'weekday'] == 1)|(dataset.loc[row, 'weekday'] == 6)):
      dataset.loc[row, 'naive'] = dataset.loc[row - 7*24, column_name]
    else:
      dataset.loc[row, 'naive'] = dataset.loc[row - 24, column_name]
  return dataset

gefcom_data = naive_forecast(gefcom_data, 'LMP', 360)

plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_data.loc[360*24:,'LMP'], color='b', label='LMP')
plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_data.loc[360*24:,'naive'], color='m', alpha=0.7, label='forecast')
plt.title('Naive method for GEFCOM dataset')
plt.xticks(gefcom_data.loc[360*24::168*7,'date'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('LMP')
plt.legend()
plt.show()

np_data['weekday'] = np_data['date'].apply(lambda x: (x + dt.timedelta(days=1)).dayofweek)

np_data = naive_forecast(np_data, 'system price', 360)

plt.plot(np_data.loc[360*24:,'date'], np_data.loc[360*24:,'system price'], color='b', label='price')
plt.plot(np_data.loc[360*24:,'date'], np_data.loc[360*24:,'naive'], color='m', alpha=0.7, label='forecast')
plt.title('Naive method for Nord Pool dataset')
plt.xticks(np_data.loc[360*24::168*7,'date'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('System price')
plt.legend()
plt.show()

"""* Expert

$q_{d,h} = \beta_{h,1} q_{d−1,h} + \beta_{h,2}q_{d−2,h} + \beta_{h,3} q_{d−7,h} + \beta_{h,4} q_{d−1,min} + \beta_{h,5} z_t + \sum_{i=1}^{3} \beta_{h,i+5} D_i + \epsilon_{d,h}$
"""

import statsmodels.formula.api as smf

def expert_model(dataset, column_names, forecast_window):

  # data_model['ml_price'] = dataset['ml_price']
  forecast_array = []
  for ind in range(0, int(len(dataset)/24)-forecast_window):
    ml_price = np.mean(np.log(dataset.loc[ind*24:forecast_window*24+ind*24, column_names[0]]))
    dataset['ml_price'] = np.log(dataset[column_names[0]]) - ml_price
    data_model = pd.DataFrame({
      'q_d1': dataset['ml_price'].shift(1*24),
      'q_d2': dataset['ml_price'].shift(2*24),
      'q_d7': dataset['ml_price'].shift(7*24),
      'q_d1_min': pd.Series(dataset.groupby(dataset['date'])[column_names[0]].min().shift()).repeat(24).reset_index(drop=True),
      'Zt': np.log(dataset[column_names[1]]),
      'D1': np.where(dataset['weekday']==1, 1, 0),
      'D2': np.where(dataset['weekday']==6, 1, 0),
      'D3': np.where(dataset['weekday']==0, 1, 0)
      })
    data_model['q_d1_min'] = np.log(data_model['q_d1_min']) - ml_price
    dataset_train, dataset_observation = data_model[ind*24:forecast_window*24+ind*24], data_model.loc[forecast_window*24+ind*24:forecast_window*24+(ind+1)*24-1]
    X = dataset_train[7*24:]
    X = X.to_numpy()
    Y = dataset.loc[ind*24+7*24:forecast_window*24+ind*24-1, 'ml_price']

    beta = np.linalg.lstsq(X, Y, rcond=None)[0]
    prognosis = np.dot(beta, dataset_observation.T)
    forecast_array.extend(np.exp(prognosis + ml_price))
  return forecast_array

gefcom_expert_forecast = expert_model(gefcom_data, ['LMP', 'day-ahead system load'], 360)

plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_data.loc[360*24:,'LMP'], color='b', label='LMP')
plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_expert_forecast, color='m', alpha=0.7, label='forecast')
plt.title('Expert method for GEFCOM dataset')
plt.xticks(gefcom_data.loc[360*24::168*4,'date'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('LMP')
plt.legend()
plt.show()

plt.plot(gefcom_data.loc[24*360:24*360+167, 'datetime'], gefcom_data.loc[24*360:24*360+167, 'LMP'], label='LMP')
plt.plot(gefcom_data.loc[24*360:24*360+167, 'datetime'], gefcom_expert_forecast[:168], color='m', label='forecast')
plt.title('Expert method for GEFCOM dataset - first week predictions')
plt.xticks(gefcom_data.loc[24*360:24*360+168:24, 'datetime'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('LMP')
plt.legend()
plt.show()

np_expert_forecast = expert_model(np_data, ['system price', 'consumption prognosis'], 360)

plt.plot(np_data.loc[360*24:,'datetime'], np_data.loc[360*24:,'system price'], color='b', label='price')
plt.plot(np_data.loc[360*24:,'datetime'], np_expert_forecast, color='m', alpha=0.8, label='forecast')
plt.title('Expert method for Nord Pool dataset')
plt.xlabel('Date')
plt.ylabel('System price')
plt.legend()
plt.show()

"""### The seasonal component autoregressive (SCAR) benchmarks"""

def decompose_series(series):
    # result = seasonal_decompose(series, period=360)
    cycle, trend = hpfilter(series, lamb=10**8)
    return cycle, trend


# create log-prices
log_prices = np.log(np_data['system price'].values)
exogenous_series = np.log(np_data['consumption prognosis'].values)

calibration_window = 360 * 24

# Step 1(a): Decompose log-prices series
seasonal_component, trend_component = decompose_series(log_prices[180*24:180*24+360*24])
trend_forecast = trend_component[-24:]

# Step 1(b): Decompose exogenous series
exog_seasonal_component, exog_trend_component = decompose_series(exogenous_series[180*24:180*24+(360+1)*24])

# Remove trend
log_prices_train = log_prices[180*24:180*24+360*24] - trend_component
log_exog_train = exogenous_series[180*24:180*24+(360+1)*24] - exog_trend_component

plt.plot(np_data.loc[180*24+360*24-48+1:180*24+360*24+24, 'datetime'], log_prices[180*24+360*24-48:180*24+360*24+24], color='gray',
         marker='o', ls='solid', label=r'$p_{d,h}$')
plt.plot(np_data.loc[180*24+360*24-48:180*24+360*24-1, 'datetime'], trend_component[-48:], color='r', label=r'$T_{d,h} HP(10^8)$')
plt.scatter(np_data.loc[180*24+360*24-24:180*24+360*24-1, 'datetime'], trend_component[-24:], color='r', label=r'$T_{d^*,h}$')
plt.scatter(np_data.loc[180*24+360*24:180*24+360*24+24-1, 'datetime'], trend_forecast, color='b', label=r'$\hat{T}_{{d+1}^*,h}$')

plt.legend()
plt.title('Nord Pool prices with trend line')
plt.ylabel('Nord Pool log-prices')
plt.xlabel('Date')
plt.show()

def scar_model(dataset, column_names, forecast_window, horizon=False):

  forecast_array = []
  if not horizon:
    horizon = int(len(dataset)/24)-forecast_window
  for ind in range(0, horizon):
    ml_price = np.mean(dataset.loc[ind*24:forecast_window*24+ind*24, column_names[0]])
    dataset['ml_price'] = dataset[column_names[0]] - ml_price
    # dataset['ml_price'] = dataset[column_names[0]]
    data_model = pd.DataFrame({
      'q_d1': dataset['ml_price'].shift(1*24),
      'q_d2': dataset['ml_price'].shift(2*24),
      'q_d7': dataset['ml_price'].shift(7*24),
      'q_d1_min': pd.Series(dataset.groupby(dataset['date'])[column_names[0]].min().shift()).repeat(24).reset_index(drop=True),
      'Zt': dataset[column_names[1]],
      'D1': np.where(dataset['weekday'] == 1, 1, 0),
      'D2': np.where(dataset['weekday'] == 6, 1, 0),
      'D3': np.where(dataset['weekday'] == 0, 1, 0)
      })
    data_model['q_d1_min'] = data_model['q_d1_min'] - ml_price
    dataset_train, dataset_observation = data_model[ind*24:forecast_window*24+ind*24], data_model.loc[forecast_window*24+ind*24:forecast_window*24+(ind+1)*24-1]
    X = dataset_train[7*24:]
    X = X.to_numpy()
    Y = dataset.loc[ind*24+7*24:forecast_window*24+ind*24-1, 'ml_price']

    beta = np.linalg.lstsq(X, Y, rcond=None)[0]
    prognosis = np.dot(beta, dataset_observation.T)
    forecast_array.extend(prognosis)
  return forecast_array

df_to_ARX = np_data.loc[:8663, 'date'].copy().to_frame(name='date')
df_to_ARX.loc[:8639, 'price'] = log_prices_train.tolist()
df_to_ARX.loc[:8663, 'exog'] = log_exog_train.tolist()
df_to_ARX.loc[:8663, 'weekday'] = np_data.loc[:8663, 'weekday']

arx_forecasts = scar_model(df_to_ARX, ['price', 'exog'], 360, 1)
# Step 3: Add ARX forecasts to persistent forecasts of LTSC
log_price_forecasts = trend_component[-24:] + arx_forecasts

# Step 4: Convert log-price forecasts into price forecasts
price_forecasts = np.exp(log_price_forecasts)

# Display or use the price forecasts as needed
print("Price Forecasts for the next 24 hours:", price_forecasts)

plt.plot(np_data.loc[8664:8663+24, 'datetime'], np_data.loc[360*24+1:361*24, 'system price'], color='b', label='price')
plt.plot(np_data.loc[8664:8663+24, 'datetime'], price_forecasts, color='m', alpha=0.7, label='forecast')
plt.title('SCAR method for Nord Pool dataset  - one day predictions')
plt.ylabel('System price')
plt.xlabel('Date')
plt.legend()
plt.show()

"""--

--

###  The artificial neural network (ANN) benchmark
"""

def narx_model(dataset, column_names, forecast_window, output_file):
  import keras
  import numpy as np
  from keras.layers import Input, Dense

  forecast_array = []
  with open(output_file, "ab") as f:
    for ind in range(0, int(len(dataset)/24)-forecast_window):
      ml_price = np.mean(np.log(dataset.loc[ind*24:forecast_window*24+ind*24, column_names[0]]))
      dataset['ml_price'] = np.log(dataset[column_names[0]]) - ml_price
      data_model = pd.DataFrame({
        'q_d1': dataset['ml_price'].shift(1*24),
        'q_d2': dataset['ml_price'].shift(2*24),
        'q_d7': dataset['ml_price'].shift(7*24),
        'q_d1_min': pd.Series(dataset.groupby(dataset['date'])[column_names[0]].min().shift()).repeat(24).reset_index(
            drop=True),
        'Zt': np.log(dataset[column_names[1]]),
        'D1': np.where(dataset['weekday']==1, 1, 0),
        'D2': np.where(dataset['weekday']==6, 1, 0),
        'D3': np.where(dataset['weekday']==0, 1, 0)
        })
      data_model['q_d1_min'] = np.log(data_model['q_d1_min']) - ml_price
      dataset_train, dataset_observation = data_model[ind*24:forecast_window*24+ind*24], data_model.loc[forecast_window*24+ind*24:forecast_window*24+(ind+1)*24-1]
      X = dataset_train[7*24:]
      X = X.to_numpy()
      Y = dataset.loc[ind*24+7*24:forecast_window*24+ind*24-1, 'ml_price']

      # beta = np.linalg.lstsq(X, Y, rcond=None)[0]
      # prognosis = np.dot(beta, dataset_observation.T)

      # Define Neural Network model
      inputs = Input(shape=(X.shape[1], ))                  # Input layer
      hidden = Dense(units=5, activation='sigmoid')(inputs)# Hidden layer (5 neurons; GM = 20)
      outputs = Dense(units=1, activation='linear')(hidden) # Output layer
      model = keras.Model(inputs=inputs, outputs=outputs)
      # callbacks = [EarlyStopping(patience=20, restore_best_weights=True)]
      callbacks = []
      model.compile(loss='MAE', optimizer='ADAM')           # Compile model
      model.fit(X, Y, batch_size=64, epochs=500, verbose=0, # Fit to data
                validation_split=.0, shuffle=False, callbacks=callbacks)
      prognosis = model.predict(np.array(dataset_observation, ndmin=2))        # Compute a step-ahead forecast

      forecast_array.extend(np.exp(prognosis + ml_price))
      np.savetxt(f, np.exp(prognosis + ml_price))
      f.write(b"\n")
  return forecast_array

# gefcom_narx_forecast = narx_model(gefcom_data, ['LMP', 'day-ahead system load'], 360, 'gefcom_narx.txt')
# super long execution

gefcom_narx= pd.read_table('gefcom_narx_final.txt', sep="\t", names=['forecast'])

plt.plot(gefcom_data.loc[360*24:, 'datetime'], gefcom_narx, color='b', label='LMP')
plt.plot(gefcom_data.loc[360*24:, 'datetime'], gefcom_data.loc[360*24:, 'LMP'], color='m', alpha=0.7, label='forecast')
plt.ylabel('LMP')
plt.xlabel('Date')
plt.title('ANN benchmark for gefcom data')
plt.legend()
plt.show()

# gefcom_narx_forecast

# np.savetxt('gefcom_narx.txt', gefcom_narx_forecast)
gefcom_narx_forecast = np.loadtxt('gefcom_narx_final.txt')

plt.plot(gefcom_data.loc[360*24:,'LMP'], color='b')
plt.plot(gefcom_narx_forecast, color='m')

np_narx_forecast = narx_model(np_data, ['system price', 'consumption prognosis'], 360)

np.savetxt('np_narx.txt', np_narx_forecast)

plt.plot(np_data.loc[360*24:,'system price'], color='b')
plt.plot(np_expert_forecast, color='m')

"""### Committee machines of ANN networks"""



"""### Committee machines of SCANN networks

"""



"""## Empirical results

### Performance evaluation in terms of WMAE

$WMAE = \frac{1}{\overline{P}_{168}} MAE_i=\frac{1}{168\cdot \overline{P}_{168}}\sum_{d=1}^{7}\sum_{h=1}^{24} |P_{d,h}-\hat{P}_{d, h}^i|$

where Pd,h is the actual price for day d and hour h (not
the log-price pd,h), Pˆi d,h is the predicted price for that day and hour obtained from model i, and P¯168 is the mean price for a given week. Note that WMAE requires the test period to be a multiple of a week (or 168 h). Hence, when computing WMAE, we consider the first 103 full weeks (27.12.2011–16.12.2013) for the GEFCom2014 dataset and the first 104 full weeks (27.12.2013 24.12.2015) for the Nord Pool dataset.
"""

def wmae(actual_values, forecast):
    number_weeks = int(len(actual_values)/168)
    print(number_weeks)
    wmae = []
    for i in range(number_weeks):
        wmae.append(1/(168*np.mean(actual_values[i*168:i*168+168]))*sum(np.abs(actual_values[i*168:i*168+168] - forecast[i*168:i*168+168])))
    return wmae

gefcom_data[360*24:1081*24]

wmae_naive_gefcom_data = wmae(gefcom_data.loc[360*24:1081*24-1, 'LMP'], gefcom_data.loc[360*24:1081*24-1, 'naive'])

np.mean(wmae_naive_gefcom_data)*100

wmae_naive_np_data = wmae(np_data.loc[360*24:1088*24-1, 'system price'], np_data.loc[360*24:1088*24-1, 'naive'])

np.mean(wmae_naive_np_data)*100

wmae_expert_gefcom_data = wmae(gefcom_data.loc[360*24:1081*24-1, 'LMP'], gefcom_expert_forecast[:103*24*7])

np.mean(wmae_expert_gefcom_data)*100

wmae_expert_np_data = wmae(np_data.loc[360*24:1088*24-1, 'system price'], np_expert_forecast[:104*24*7])

np.mean(wmae_expert_np_data)*100

pip install tabulate

from tabulate import tabulate

table = [['Model for gefcom', 'WMAE'],
         ['Naïve', np.mean(wmae_naive_gefcom_data)*100],
         ['ARX', np.mean(wmae_expert_gefcom_data)*100],
         ['ANN_1', 1],
         ['ANN_5', 1]]

print(tabulate(table, headers='firstrow'))

table = [['Model for nord pool', 'WMAE'],
         ['Naïve', np.mean(wmae_naive_np_data)*100],
         ['ARX', np.mean(wmae_expert_np_data)*100],
         ['ANN_1', 1],
         ['ANN_5', 1]]

print(tabulate(table, headers='firstrow'))

"""### DM tests"""

gefcom_naive_forecast_103 = gefcom_data.loc[360*24:1081*24-1, 'naive']
np_naive_forecast_104 = np_data.loc[360*24:1088*24-1, 'naive']
gefcom_expert_forecast_103 = gefcom_expert_forecast[:103*24*7]
np_expert_forecast_104 = np_expert_forecast[:104*24*7]

!pip install git+https://github.com/johntwk/Diebold-Mariano-Test.git

from dm_test import dm_test
import random

rt = dm_test(gefcom_data.loc[360*24:1081*24-1, 'LMP'],gefcom_naive_forecast_103,gefcom_expert_forecast_103,h = 1, crit="MAD")
print(rt)









"""### The number of hidden neurons

"""





"""### The size of the committee machine

# Additional developement of the paper

ARIMA ROLLING WINDOW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA

# Loading GEFCOM data
gefcom_data = pd.read_table('GEFCOM.txt', sep="\t", header=None, usecols=[0,1,2,3,4,5],
                            names=['date', 'hour', 'LMP', 'day-ahead zonal load', 'day-ahead system load', 'weekday'], engine='python')

# Loading Nord Pool data
np_data = pd.read_table('NPdata_2013-2016.txt', sep='\s+', header=None, usecols=[0,1,2,3],
                        names=['date', 'hour', 'system price', 'consumption prognosis', 'weekday'], engine='python')

# Convert date columns to datetime format
gefcom_data['date'] = pd.to_datetime(gefcom_data['date'], format='%Y%m%d')
np_data['date'] = pd.to_datetime(np_data['date'], format='%Y%m%d')

def arima_forecast(dataset, column_name, forecast_window, order=(1,1,1)):
    dataset_train, dataset_test = dataset[:forecast_window*24], dataset[forecast_window*24:]
    if 'arima' in dataset.columns:
        del dataset['arima']
    dataset.insert(len(dataset.columns), 'arima', np.nan)
    for row in range(forecast_window*24, forecast_window*24 + len(dataset_test)):
        model = ARIMA(dataset.loc[:row-1, column_name].values, order=order)
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=1)
        dataset.loc[row, 'arima'] = forecast[0]
    return dataset

gefcom_data = arima_forecast(gefcom_data, 'LMP', 360)

plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_data.loc[360*24:,'LMP'], color='b', label='LMP')
plt.plot(gefcom_data.loc[360*24:,'date'], gefcom_data.loc[360*24:,'arima'], color='m', alpha=0.7, label='forecast')
plt.title('ARIMA method for GEFCOM dataset')
plt.xticks(gefcom_data.loc[360*24::168*7,'date'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('LMP')
plt.legend()
plt.show()

np_data = arima_forecast(np_data, 'system price', 360)

plt.plot(np_data.loc[360*24:,'date'], np_data.loc[360*24:,'system price'], color='b', label='price')
plt.plot(np_data.loc[360*24:,'date'], np_data.loc[360*24:,'arima'], color='m', alpha=0.7, label='forecast')
plt.title('ARIMA method for Nord Pool dataset')
plt.xticks(np_data.loc[360*24::168*7,'date'], rotation ='vertical')
plt.xlabel('Date')
plt.ylabel('System price')
plt.legend()
plt.show()

